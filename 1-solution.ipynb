{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import textract\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to import the packages needed:\n",
    "- `textract` for extracting the text from the PDF file\n",
    "- Python `regular expression` library (`re`)\n",
    "- `pandas` library for storing the data in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = textract.process('EUtaxonomy.pdf', method='pdfminer').decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textract is a library which deals with all the complications of extracting text from PDF files. This could be improved by developing a better extractor tailored specifically for this document but that is out of the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = re.split(r\"\\s*?\\n\\s*?\\n\\s*?\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the part which the large corpus is split into paragraphs using regular expressions (regex).\n",
    "The regex above is splitting all the text sections that are separated by several newlines and spaces. This is another part which could be developed to improve the segmentation of the texts/paragraphs, however, a split as such is sufficient for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(raw_text):\n",
    "    raw_text = raw_text.replace(\"\\n\", \" \").replace(\"  \", \" \").strip(\" \")\n",
    "    return re.sub(r'[^\\w\\s]', '', raw_text).strip(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built a function for cleaning each paragraph which will replace newlines and replace double spaces. The last regex is removing metacharaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PARAGRAPH_CHARACTERS = 200  # Can adjust this value\n",
    "paragraphs = []\n",
    "for text_section in split_text:\n",
    "    clean_text = cleaning_text(text_section)\n",
    "    if len(clean_text) >= MIN_PARAGRAPH_CHARACTERS:\n",
    "        paragraphs.append(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the paragraphs and claening them and omitting paragraphs that are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"paragraph\": paragraphs})\n",
    "df.to_csv(\"paragraphs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we store the paragraphs as a pandas dataframe and save it as `.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
